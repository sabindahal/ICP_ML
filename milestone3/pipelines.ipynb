{
 "cells": [
  {
   "cell_type": "code",
   "id": "4a94368c48a7c528",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T21:42:26.244617Z",
     "start_time": "2025-12-11T21:42:26.239238Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "import pytesseract\n",
    "import re\n",
    "import time\n",
    "import easyocr"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "aa9164e562ce3a87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T21:42:33.161566Z",
     "start_time": "2025-12-11T21:42:27.044121Z"
    }
   },
   "source": [
    "ds = load_dataset(\"lansinuote/ocr_id_card\")"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "2bbb752c848bc125",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T21:42:34.698950Z",
     "start_time": "2025-12-11T21:42:34.688945Z"
    }
   },
   "source": [
    "# okay time for some actual functionality: let's make an accuracy checker\n",
    "def accuracyChecker(words, testData):\n",
    "    name = testData[0]['word']\n",
    "    accuracy = 0\n",
    "    yearFound = False\n",
    "    monthFound = False\n",
    "    dayFound = False\n",
    "    IDFound = False\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        if words[i] == name:\n",
    "            accuracy += 1\n",
    "        if re.fullmatch(r\"\\d{4}\", words[i]) and not yearFound:\n",
    "            if 2025 > int(words[i]) >= 1900:\n",
    "                accuracy += 1\n",
    "                yearFound = True\n",
    "        if re.fullmatch(r\"\\d{1,2}\", words[i]) and not monthFound:\n",
    "            if 12 > int(words[i]) >= 1:\n",
    "                accuracy += 1\n",
    "                monthFound = True\n",
    "        if re.fullmatch(r\"\\d{1,2}\", words[i]) and not dayFound:\n",
    "            if 31 > int(words[i]) >= 1:\n",
    "                accuracy += 1\n",
    "                dayFound = True\n",
    "        if re.fullmatch(r\"\\d{18}\", words[i]) and not IDFound:\n",
    "            accuracy += 1\n",
    "            IDFound = True\n",
    "    return accuracy / 5\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "980f7ae7061b9a01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T21:50:05.848957Z",
     "start_time": "2025-12-11T21:49:54.075149Z"
    }
   },
   "source": [
    "# okay gonna run all the data through the whole pipeline, this should make it possible to change a pipeline and compute average accuracy of that pipeline\n",
    "\n",
    "accuracy = []\n",
    "latency = []\n",
    "numImages = len(ds['train'])\n",
    "numImages = 10\n",
    "for imgNum in range(numImages):\n",
    "    # collect an image from the dataset and turn it into an array for edge detection, use greyscale\n",
    "    currImg = ds['train'][imgNum]['image']\n",
    "    testingImageArray = np.array(currImg)\n",
    "    edgeDetectionArray = cv2.cvtColor(testingImageArray, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # apply canny edge detection\n",
    "    edges = cv2.Canny(edgeDetectionArray, 100, 200)\n",
    "\n",
    "    # do a hough lines transform\n",
    "    lines = cv2.HoughLinesP(edges, 1, np.pi / 180, 75, None, 50, 10)\n",
    "    linesTest = np.zeros(edges.shape) + 250\n",
    "    for i in range(0, len(lines)):\n",
    "        l = lines[i][0]\n",
    "        cv2.line(linesTest, (l[0], l[1]), (l[2], l[3]), (0,0,255), 3, cv2.LINE_AA)\n",
    "\n",
    "    # find the angles at which each line lies\n",
    "    theta = np.zeros(len(lines))\n",
    "    for i in range(0, len(lines)):\n",
    "        l = lines[i][0]\n",
    "        if abs(l[3] - l[1]) == 0:\n",
    "            # horizontal line\n",
    "            theta[i] = 0\n",
    "        else:\n",
    "            theta[i] = math.atan(abs(l[2] - l[0])/abs(l[3] - l[1]))\n",
    "\n",
    "    # find smallest angle greater than pi/4\n",
    "    optTheta = 100\n",
    "    optIndex = 0\n",
    "    for k in range(len(theta)):\n",
    "        if theta[k] > np.pi/4:\n",
    "            if theta[k] < optTheta:\n",
    "                optTheta = theta[k]\n",
    "                optIndex = k\n",
    "    mostHorz = np.float32(lines[optIndex][0])\n",
    "    # this actually ends up being the least horizontal of the still pretty horizontal lines, deals with noise from extra lines created from the text\n",
    "\n",
    "    # find some points to warp the image, use the midpoint of the lines offset a bit, and the new destination of the endpoints of the least horizontal but still horizontal line\n",
    "    mdpnt = [(mostHorz[0] + mostHorz[2])/2,(mostHorz[1] + mostHorz[3])/2]\n",
    "    adjustment = 100\n",
    "    if mdpnt[1] > 250:\n",
    "        initialPts = np.float32([[mostHorz[0], mostHorz[1]],\n",
    "                                 [mostHorz[2], mostHorz[3]],\n",
    "                                 [mdpnt[0], mdpnt[1] - adjustment]])\n",
    "        finalPts = np.float32([[mostHorz[0], mdpnt[1]],\n",
    "                               [mostHorz[2], mdpnt[1]],\n",
    "                               [mdpnt[0], mdpnt[1] - adjustment]])\n",
    "    else:\n",
    "        initialPts = np.float32([[mostHorz[0], mostHorz[1]],\n",
    "                                 [mostHorz[2], mostHorz[3]],\n",
    "                                 [mdpnt[0], mdpnt[1] + adjustment]])\n",
    "        finalPts = np.float32([[mostHorz[0], mdpnt[1]],\n",
    "                               [mostHorz[2], mdpnt[1]],\n",
    "                               [mdpnt[0], mdpnt[1] + adjustment]])\n",
    "\n",
    "    # warp it\n",
    "    M = cv2.getAffineTransform(initialPts, finalPts)\n",
    "    warpedImg = cv2.warpAffine(testingImageArray, M, (testingImageArray.shape[1], testingImageArray.shape[0]))\n",
    "\n",
    "    # red it\n",
    "    redshiftImg = warpedImg\n",
    "    for m in range(warpedImg.shape[0]):\n",
    "        for n in range(warpedImg.shape[1]):\n",
    "            redshiftImg[m][n][1] = 0\n",
    "            redshiftImg[m][n][2] = 0\n",
    "\n",
    "    # tesseract it, also find latency\n",
    "    start = time.time()\n",
    "    wordsRaw = pytesseract.image_to_string(redshiftImg, lang='chi_sim')\n",
    "    stop = time.time()\n",
    "    latency.append(stop - start)\n",
    "\n",
    "    # clean and tokenize the data from OCR\n",
    "    words = wordsRaw.replace(\" \",\"\").splitlines()\n",
    "    testingData = ds['train'][imgNum]['ocr']\n",
    "\n",
    "    pattern = r'\\d+|[\\u4e00-\\u9fff]+|[A-Za-z]+|\\s|[^\\w\\s]'\n",
    "    tokens = []\n",
    "    for i in range(len(words)):\n",
    "        tokens.append(re.findall(pattern, words[i]))\n",
    "\n",
    "    # flatten it out\n",
    "    wordsData = []\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens[i])):\n",
    "            wordsData.append(tokens[i][j])\n",
    "\n",
    "    # run the accuracy checker on the final data\n",
    "    accuracy.append(accuracyChecker(wordsData, testingData))\n",
    "\n",
    "print(\"Average accuracy: \", np.mean(accuracy))\n",
    "print(\"Average latency: \", np.mean(latency))\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy:  0.5\n",
      "Average latency:  0.960825252532959\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T21:53:12.735703Z",
     "start_time": "2025-12-11T21:52:45.939540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# here will be an easyOCR pipeline\n",
    "\n",
    "accuracy = []\n",
    "latency = []\n",
    "numImages = len(ds['train'])\n",
    "numImages = 10\n",
    "reader = easyocr.Reader(['ch_sim'])\n",
    "for imgNum in range(numImages):\n",
    "    # collect an image from the dataset and turn it into an array for edge detection, use greyscale\n",
    "    currImg = ds['train'][imgNum]['image']\n",
    "    testingImageArray = np.array(currImg)\n",
    "    edgeDetectionArray = cv2.cvtColor(testingImageArray, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # apply canny edge detection\n",
    "    edges = cv2.Canny(edgeDetectionArray, 100, 200)\n",
    "\n",
    "    # do a hough lines transform\n",
    "    lines = cv2.HoughLinesP(edges, 1, np.pi / 180, 75, None, 50, 10)\n",
    "    linesTest = np.zeros(edges.shape) + 250\n",
    "    for i in range(0, len(lines)):\n",
    "        l = lines[i][0]\n",
    "        cv2.line(linesTest, (l[0], l[1]), (l[2], l[3]), (0,0,255), 3, cv2.LINE_AA)\n",
    "\n",
    "    # find the angles at which each line lies\n",
    "    theta = np.zeros(len(lines))\n",
    "    for i in range(0, len(lines)):\n",
    "        l = lines[i][0]\n",
    "        if abs(l[3] - l[1]) == 0:\n",
    "            # horizontal line\n",
    "            theta[i] = 0\n",
    "        else:\n",
    "            theta[i] = math.atan(abs(l[2] - l[0])/abs(l[3] - l[1]))\n",
    "\n",
    "    # find smallest angle greater than pi/4\n",
    "    optTheta = 100\n",
    "    optIndex = 0\n",
    "    for k in range(len(theta)):\n",
    "        if theta[k] > np.pi/4:\n",
    "            if theta[k] < optTheta:\n",
    "                optTheta = theta[k]\n",
    "                optIndex = k\n",
    "    mostHorz = np.float32(lines[optIndex][0])\n",
    "    # this actually ends up being the least horizontal of the still pretty horizontal lines, deals with noise from extra lines created from the text\n",
    "\n",
    "    # find some points to warp the image, use the midpoint of the lines offset a bit, and the new destination of the endpoints of the least horizontal but still horizontal line\n",
    "    mdpnt = [(mostHorz[0] + mostHorz[2])/2,(mostHorz[1] + mostHorz[3])/2]\n",
    "    adjustment = 100\n",
    "    if mdpnt[1] > 250:\n",
    "        initialPts = np.float32([[mostHorz[0], mostHorz[1]],\n",
    "                                 [mostHorz[2], mostHorz[3]],\n",
    "                                 [mdpnt[0], mdpnt[1] - adjustment]])\n",
    "        finalPts = np.float32([[mostHorz[0], mdpnt[1]],\n",
    "                               [mostHorz[2], mdpnt[1]],\n",
    "                               [mdpnt[0], mdpnt[1] - adjustment]])\n",
    "    else:\n",
    "        initialPts = np.float32([[mostHorz[0], mostHorz[1]],\n",
    "                                 [mostHorz[2], mostHorz[3]],\n",
    "                                 [mdpnt[0], mdpnt[1] + adjustment]])\n",
    "        finalPts = np.float32([[mostHorz[0], mdpnt[1]],\n",
    "                               [mostHorz[2], mdpnt[1]],\n",
    "                               [mdpnt[0], mdpnt[1] + adjustment]])\n",
    "\n",
    "    # warp it\n",
    "    M = cv2.getAffineTransform(initialPts, finalPts)\n",
    "    warpedImg = cv2.warpAffine(testingImageArray, M, (testingImageArray.shape[1], testingImageArray.shape[0]))\n",
    "\n",
    "    # red it\n",
    "    redshiftImg = warpedImg\n",
    "    for m in range(warpedImg.shape[0]):\n",
    "        for n in range(warpedImg.shape[1]):\n",
    "            redshiftImg[m][n][1] = 0\n",
    "            redshiftImg[m][n][2] = 0\n",
    "\n",
    "    # easyocr it\n",
    "    start = time.time()\n",
    "    results = reader.readtext(redshiftImg)\n",
    "    stop = time.time()\n",
    "    latency.append(stop - start)\n",
    "\n",
    "    # snag the words\n",
    "    wordsRaw = []\n",
    "    for res in results:\n",
    "        wordsRaw.append(res[1])\n",
    "\n",
    "    # clean and tokenize the data from OCR\n",
    "    words = []\n",
    "    for w in wordsRaw:\n",
    "        w_clean = w.replace(\" \", \"\")\n",
    "        words.append(w_clean)\n",
    "\n",
    "    testingData = ds['train'][imgNum]['ocr']\n",
    "\n",
    "    pattern = r'\\d+|[\\u4e00-\\u9fff]+|[A-Za-z]+|\\s|[^\\w\\s]'\n",
    "    tokens = []\n",
    "    for i in range(len(words)):\n",
    "        tokens.append(re.findall(pattern, words[i]))\n",
    "\n",
    "    # flatten it out\n",
    "    wordsData = []\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens[i])):\n",
    "            wordsData.append(tokens[i][j])\n",
    "\n",
    "    # run the accuracy checker on the final data\n",
    "    accuracy.append(accuracyChecker(wordsData, testingData))\n",
    "\n",
    "print(\"Average accuracy: \", np.mean(accuracy))\n",
    "print(\"Average latency: \", np.mean(latency))\n",
    "\n"
   ],
   "id": "aed99a0976661755",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy:  0.0\n",
      "Average latency:  2.148895239830017\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
