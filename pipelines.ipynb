{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T23:08:33.137019Z",
     "start_time": "2025-09-28T23:08:16.296713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "import pytesseract\n",
    "import re\n",
    "import time"
   ],
   "id": "4a94368c48a7c528",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Max's PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T23:08:46.921961Z",
     "start_time": "2025-09-28T23:08:36.755809Z"
    }
   },
   "cell_type": "code",
   "source": "ds = load_dataset(\"lansinuote/ocr_id_card\")",
   "id": "aa9164e562ce3a87",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T23:18:08.399794Z",
     "start_time": "2025-09-28T23:18:08.376715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# okay time for some actual functionality: let's make an accuracy checker\n",
    "def accuracyChecker(words, testData):\n",
    "    name = testData[0]['word']\n",
    "    accuracy = 0\n",
    "    yearFound = False\n",
    "    monthFound = False\n",
    "    dayFound = False\n",
    "    IDFound = False\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        if words[i] == name:\n",
    "            accuracy += 1\n",
    "        if re.fullmatch(r\"\\d{4}\", words[i]) and not yearFound:\n",
    "            if 2025 > int(words[i]) >= 1900:\n",
    "                accuracy += 1\n",
    "                yearFound = True\n",
    "        if re.fullmatch(r\"\\d{1,2}\", words[i]) and not monthFound:\n",
    "            if 12 > int(words[i]) >= 1:\n",
    "                accuracy += 1\n",
    "                monthFound = True\n",
    "        if re.fullmatch(r\"\\d{1,2}\", words[i]) and not dayFound:\n",
    "            if 31 > int(words[i]) >= 1:\n",
    "                accuracy += 1\n",
    "                dayFound = True\n",
    "        if re.fullmatch(r\"\\d{18}\", words[i]) and not IDFound:\n",
    "            accuracy += 1\n",
    "            IDFound = True\n",
    "    return accuracy / 5\n"
   ],
   "id": "2bbb752c848bc125",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T23:16:49.988134Z",
     "start_time": "2025-09-28T23:16:44.073600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# okay gonna run all the data through the whole pipeline, this should make it possible to change a pipeline and compute average accuracy of that pipeline\n",
    "\n",
    "accuracy = []\n",
    "latency = []\n",
    "numImages = len(ds['train'])\n",
    "numImages = 5\n",
    "for imgNum in range(numImages):\n",
    "    # collect an image from the dataset and turn it into an array for edge detection, use greyscale\n",
    "    currImg = ds['train'][imgNum]['image']\n",
    "    testingImageArray = np.array(currImg)\n",
    "    edgeDetectionArray = cv2.cvtColor(testingImageArray, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # apply canny edge detection\n",
    "    edges = cv2.Canny(edgeDetectionArray, 100, 200)\n",
    "\n",
    "    # do a hough lines transform\n",
    "    lines = cv2.HoughLinesP(edges, 1, np.pi / 180, 75, None, 50, 10)\n",
    "    linesTest = np.zeros(edges.shape) + 250\n",
    "    for i in range(0, len(lines)):\n",
    "        l = lines[i][0]\n",
    "        cv2.line(linesTest, (l[0], l[1]), (l[2], l[3]), (0,0,255), 3, cv2.LINE_AA)\n",
    "\n",
    "    # find the angles at which each line lies\n",
    "    theta = np.zeros(len(lines))\n",
    "    for i in range(0, len(lines)):\n",
    "        l = lines[i][0]\n",
    "        if abs(l[3] - l[1]) == 0:\n",
    "            # horizontal line\n",
    "            theta[i] = 0\n",
    "        else:\n",
    "            theta[i] = math.atan(abs(l[2] - l[0])/abs(l[3] - l[1]))\n",
    "\n",
    "    # find smallest angle greater than pi/4\n",
    "    optTheta = 100\n",
    "    optIndex = 0\n",
    "    for k in range(len(theta)):\n",
    "        if theta[k] > np.pi/4:\n",
    "            if theta[k] < optTheta:\n",
    "                optTheta = theta[k]\n",
    "                optIndex = k\n",
    "    mostHorz = np.float32(lines[optIndex][0])\n",
    "    # this actually ends up being the least horizontal of the still pretty horizontal lines, deals with noise from extra lines created from the text\n",
    "\n",
    "    # find some points to warp the image, use the midpoint of the lines offset a bit, and the new destination of the endpoints of the least horizontal but still horizontal line\n",
    "    mdpnt = [(mostHorz[0] + mostHorz[2])/2,(mostHorz[1] + mostHorz[3])/2]\n",
    "    adjustment = 100\n",
    "    if mdpnt[1] > 250:\n",
    "        initialPts = np.float32([[mostHorz[0], mostHorz[1]],\n",
    "                                 [mostHorz[2], mostHorz[3]],\n",
    "                                 [mdpnt[0], mdpnt[1] - adjustment]])\n",
    "        finalPts = np.float32([[mostHorz[0], mdpnt[1]],\n",
    "                               [mostHorz[2], mdpnt[1]],\n",
    "                               [mdpnt[0], mdpnt[1] - adjustment]])\n",
    "    else:\n",
    "        initialPts = np.float32([[mostHorz[0], mostHorz[1]],\n",
    "                                 [mostHorz[2], mostHorz[3]],\n",
    "                                 [mdpnt[0], mdpnt[1] + adjustment]])\n",
    "        finalPts = np.float32([[mostHorz[0], mdpnt[1]],\n",
    "                               [mostHorz[2], mdpnt[1]],\n",
    "                               [mdpnt[0], mdpnt[1] + adjustment]])\n",
    "\n",
    "    # warp it\n",
    "    M = cv2.getAffineTransform(initialPts, finalPts)\n",
    "    warpedImg = cv2.warpAffine(testingImageArray, M, (testingImageArray.shape[1], testingImageArray.shape[0]))\n",
    "\n",
    "    # red it\n",
    "    redshiftImg = warpedImg\n",
    "    for m in range(warpedImg.shape[0]):\n",
    "        for n in range(warpedImg.shape[1]):\n",
    "            redshiftImg[m][n][1] = 0\n",
    "            redshiftImg[m][n][2] = 0\n",
    "\n",
    "    # tesseract it, also find latency\n",
    "    start = time.time()\n",
    "    wordsRaw = pytesseract.image_to_string(redshiftImg, lang='chi_sim')\n",
    "    stop = time.time()\n",
    "    latency.append(stop - start)\n",
    "\n",
    "    # clean and tokenize the data from OCR\n",
    "    words = wordsRaw.replace(\" \",\"\").splitlines()\n",
    "    testingData = ds['train'][imgNum]['ocr']\n",
    "\n",
    "    pattern = r'\\d+|[\\u4e00-\\u9fff]+|[A-Za-z]+|\\s|[^\\w\\s]'\n",
    "    tokens = []\n",
    "    for i in range(len(words)):\n",
    "        tokens.append(re.findall(pattern, words[i]))\n",
    "\n",
    "    # flatten it out\n",
    "    wordsData = []\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens[i])):\n",
    "            wordsData.append(tokens[i][j])\n",
    "\n",
    "    # run the accuracy checker on the final data\n",
    "    accuracy.append(accuracyChecker(wordsData, testingData))\n",
    "\n",
    "print(np.mean(accuracy))\n",
    "print(np.mean(latency))\n",
    "\n"
   ],
   "id": "980f7ae7061b9a01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6799999999999999\n",
      "[0.6, 0.8, 0.8, 0.8, 0.4]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T23:20:33.583895Z",
     "start_time": "2025-09-28T23:20:28.917577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# okay try another pipeline, this will be greyscale instead of redshift\n",
    "\n",
    "accuracy = []\n",
    "latency = []\n",
    "numImages = len(ds['train'])\n",
    "numImages = 5\n",
    "for imgNum in range(numImages):\n",
    "    # collect an image from the dataset and turn it into an array for edge detection, use greyscale\n",
    "    currImg = ds['train'][imgNum]['image']\n",
    "    testingImageArray = np.array(currImg)\n",
    "    edgeDetectionArray = cv2.cvtColor(testingImageArray, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # apply canny edge detection\n",
    "    edges = cv2.Canny(edgeDetectionArray, 100, 200)\n",
    "\n",
    "    # do a hough lines transform\n",
    "    lines = cv2.HoughLinesP(edges, 1, np.pi / 180, 75, None, 50, 10)\n",
    "    linesTest = np.zeros(edges.shape) + 250\n",
    "    for i in range(0, len(lines)):\n",
    "        l = lines[i][0]\n",
    "        cv2.line(linesTest, (l[0], l[1]), (l[2], l[3]), (0,0,255), 3, cv2.LINE_AA)\n",
    "\n",
    "    # find the angles at which each line lies\n",
    "    theta = np.zeros(len(lines))\n",
    "    for i in range(0, len(lines)):\n",
    "        l = lines[i][0]\n",
    "        if abs(l[3] - l[1]) == 0:\n",
    "            # horizontal line\n",
    "            theta[i] = 0\n",
    "        else:\n",
    "            theta[i] = math.atan(abs(l[2] - l[0])/abs(l[3] - l[1]))\n",
    "\n",
    "    # find smallest angle greater than pi/4\n",
    "    optTheta = 100\n",
    "    optIndex = 0\n",
    "    for k in range(len(theta)):\n",
    "        if theta[k] > np.pi/4:\n",
    "            if theta[k] < optTheta:\n",
    "                optTheta = theta[k]\n",
    "                optIndex = k\n",
    "    mostHorz = np.float32(lines[optIndex][0])\n",
    "    # this actually ends up being the least horizontal of the still pretty horizontal lines, deals with noise from extra lines created from the text\n",
    "\n",
    "    # find some points to warp the image, use the midpoint of the lines offset a bit, and the new destination of the endpoints of the least horizontal but still horizontal line\n",
    "    mdpnt = [(mostHorz[0] + mostHorz[2])/2,(mostHorz[1] + mostHorz[3])/2]\n",
    "    adjustment = 100\n",
    "    if mdpnt[1] > 250:\n",
    "        initialPts = np.float32([[mostHorz[0], mostHorz[1]],\n",
    "                                 [mostHorz[2], mostHorz[3]],\n",
    "                                 [mdpnt[0], mdpnt[1] - adjustment]])\n",
    "        finalPts = np.float32([[mostHorz[0], mdpnt[1]],\n",
    "                               [mostHorz[2], mdpnt[1]],\n",
    "                               [mdpnt[0], mdpnt[1] - adjustment]])\n",
    "    else:\n",
    "        initialPts = np.float32([[mostHorz[0], mostHorz[1]],\n",
    "                                 [mostHorz[2], mostHorz[3]],\n",
    "                                 [mdpnt[0], mdpnt[1] + adjustment]])\n",
    "        finalPts = np.float32([[mostHorz[0], mdpnt[1]],\n",
    "                               [mostHorz[2], mdpnt[1]],\n",
    "                               [mdpnt[0], mdpnt[1] + adjustment]])\n",
    "\n",
    "    # warp it\n",
    "    M = cv2.getAffineTransform(initialPts, finalPts)\n",
    "    warpedImg = cv2.warpAffine(testingImageArray, M, (testingImageArray.shape[1], testingImageArray.shape[0]))\n",
    "\n",
    "    # grey it\n",
    "    greyshiftImg = cv2.cvtColor(warpedImg, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # tesseract it, also find latency\n",
    "    start = time.time()\n",
    "    wordsRaw = pytesseract.image_to_string(greyshiftImg, lang='chi_sim')\n",
    "    stop = time.time()\n",
    "    latency.append(stop - start)\n",
    "\n",
    "    # clean and tokenize the data from OCR\n",
    "    words = wordsRaw.replace(\" \",\"\").splitlines()\n",
    "    testingData = ds['train'][imgNum]['ocr']\n",
    "\n",
    "    pattern = r'\\d+|[\\u4e00-\\u9fff]+|[A-Za-z]+|\\s|[^\\w\\s]'\n",
    "    tokens = []\n",
    "    for i in range(len(words)):\n",
    "        tokens.append(re.findall(pattern, words[i]))\n",
    "\n",
    "    # flatten it out\n",
    "    wordsData = []\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens[i])):\n",
    "            wordsData.append(tokens[i][j])\n",
    "\n",
    "    # run the accuracy checker on the final data\n",
    "    accuracy.append(accuracyChecker(wordsData, testingData))\n",
    "\n",
    "print(np.mean(accuracy))\n",
    "print(np.mean(latency))\n",
    "\n"
   ],
   "id": "ed97595fb2e769d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.52\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T23:24:39.493256Z",
     "start_time": "2025-09-28T23:24:31.171760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# okay this time I'm gonna try the redshift image with some cleaning up of the noise\n",
    "\n",
    "accuracy = []\n",
    "latency = []\n",
    "numImages = len(ds['train'])\n",
    "numImages = 5\n",
    "for imgNum in range(numImages):\n",
    "    # collect an image from the dataset and turn it into an array for edge detection, use greyscale\n",
    "    currImg = ds['train'][imgNum]['image']\n",
    "    testingImageArray = np.array(currImg)\n",
    "    edgeDetectionArray = cv2.cvtColor(testingImageArray, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # apply canny edge detection\n",
    "    edges = cv2.Canny(edgeDetectionArray, 100, 200)\n",
    "\n",
    "    # do a hough lines transform\n",
    "    lines = cv2.HoughLinesP(edges, 1, np.pi / 180, 75, None, 50, 10)\n",
    "    linesTest = np.zeros(edges.shape) + 250\n",
    "    for i in range(0, len(lines)):\n",
    "        l = lines[i][0]\n",
    "        cv2.line(linesTest, (l[0], l[1]), (l[2], l[3]), (0,0,255), 3, cv2.LINE_AA)\n",
    "\n",
    "    # find the angles at which each line lies\n",
    "    theta = np.zeros(len(lines))\n",
    "    for i in range(0, len(lines)):\n",
    "        l = lines[i][0]\n",
    "        if abs(l[3] - l[1]) == 0:\n",
    "            # horizontal line\n",
    "            theta[i] = 0\n",
    "        else:\n",
    "            theta[i] = math.atan(abs(l[2] - l[0])/abs(l[3] - l[1]))\n",
    "\n",
    "    # find smallest angle greater than pi/4\n",
    "    optTheta = 100\n",
    "    optIndex = 0\n",
    "    for k in range(len(theta)):\n",
    "        if theta[k] > np.pi/4:\n",
    "            if theta[k] < optTheta:\n",
    "                optTheta = theta[k]\n",
    "                optIndex = k\n",
    "    mostHorz = np.float32(lines[optIndex][0])\n",
    "    # this actually ends up being the least horizontal of the still pretty horizontal lines, deals with noise from extra lines created from the text\n",
    "\n",
    "    # find some points to warp the image, use the midpoint of the lines offset a bit, and the new destination of the endpoints of the least horizontal but still horizontal line\n",
    "    mdpnt = [(mostHorz[0] + mostHorz[2])/2,(mostHorz[1] + mostHorz[3])/2]\n",
    "    adjustment = 100\n",
    "    if mdpnt[1] > 250:\n",
    "        initialPts = np.float32([[mostHorz[0], mostHorz[1]],\n",
    "                                 [mostHorz[2], mostHorz[3]],\n",
    "                                 [mdpnt[0], mdpnt[1] - adjustment]])\n",
    "        finalPts = np.float32([[mostHorz[0], mdpnt[1]],\n",
    "                               [mostHorz[2], mdpnt[1]],\n",
    "                               [mdpnt[0], mdpnt[1] - adjustment]])\n",
    "    else:\n",
    "        initialPts = np.float32([[mostHorz[0], mostHorz[1]],\n",
    "                                 [mostHorz[2], mostHorz[3]],\n",
    "                                 [mdpnt[0], mdpnt[1] + adjustment]])\n",
    "        finalPts = np.float32([[mostHorz[0], mdpnt[1]],\n",
    "                               [mostHorz[2], mdpnt[1]],\n",
    "                               [mdpnt[0], mdpnt[1] + adjustment]])\n",
    "\n",
    "    # warp it\n",
    "    M = cv2.getAffineTransform(initialPts, finalPts)\n",
    "    warpedImg = cv2.warpAffine(testingImageArray, M, (testingImageArray.shape[1], testingImageArray.shape[0]))\n",
    "\n",
    "    # red it\n",
    "    redshiftImg = warpedImg\n",
    "    for m in range(warpedImg.shape[0]):\n",
    "        for n in range(warpedImg.shape[1]):\n",
    "            redshiftImg[m][n][1] = 0\n",
    "            redshiftImg[m][n][2] = 0\n",
    "\n",
    "    # denoising\n",
    "    denoiseImg = cv2.fastNlMeansDenoising(redshiftImg, h=10)\n",
    "\n",
    "    # tesseract it, also find latency\n",
    "    start = time.time()\n",
    "    wordsRaw = pytesseract.image_to_string(denoiseImg, lang='chi_sim')\n",
    "    stop = time.time()\n",
    "    latency.append(stop - start)\n",
    "\n",
    "    # clean and tokenize the data from OCR\n",
    "    words = wordsRaw.replace(\" \",\"\").splitlines()\n",
    "    testingData = ds['train'][imgNum]['ocr']\n",
    "\n",
    "    pattern = r'\\d+|[\\u4e00-\\u9fff]+|[A-Za-z]+|\\s|[^\\w\\s]'\n",
    "    tokens = []\n",
    "    for i in range(len(words)):\n",
    "        tokens.append(re.findall(pattern, words[i]))\n",
    "\n",
    "    # flatten it out\n",
    "    wordsData = []\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens[i])):\n",
    "            wordsData.append(tokens[i][j])\n",
    "\n",
    "    # run the accuracy checker on the final data\n",
    "    accuracy.append(accuracyChecker(wordsData, testingData))\n",
    "\n",
    "print(np.mean(accuracy))\n",
    "print(np.mean(latency))\n",
    "\n",
    "# okay that was terrible, try denoising before warping\n"
   ],
   "id": "74a836030f394609",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27999999999999997\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T23:27:33.151711Z",
     "start_time": "2025-09-28T23:27:25.569842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# denoise before warping\n",
    "\n",
    "accuracy = []\n",
    "latency = []\n",
    "numImages = len(ds['train'])\n",
    "numImages = 5\n",
    "for imgNum in range(numImages):\n",
    "    # collect an image from the dataset and turn it into an array for edge detection, use greyscale\n",
    "    currImg = ds['train'][imgNum]['image']\n",
    "    testingImageArray = np.array(currImg)\n",
    "    edgeDetectionArray = cv2.cvtColor(testingImageArray, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # apply canny edge detection\n",
    "    edges = cv2.Canny(edgeDetectionArray, 100, 200)\n",
    "\n",
    "    # do a hough lines transform\n",
    "    lines = cv2.HoughLinesP(edges, 1, np.pi / 180, 75, None, 50, 10)\n",
    "    linesTest = np.zeros(edges.shape) + 250\n",
    "    for i in range(0, len(lines)):\n",
    "        l = lines[i][0]\n",
    "        cv2.line(linesTest, (l[0], l[1]), (l[2], l[3]), (0,0,255), 3, cv2.LINE_AA)\n",
    "\n",
    "    # find the angles at which each line lies\n",
    "    theta = np.zeros(len(lines))\n",
    "    for i in range(0, len(lines)):\n",
    "        l = lines[i][0]\n",
    "        if abs(l[3] - l[1]) == 0:\n",
    "            # horizontal line\n",
    "            theta[i] = 0\n",
    "        else:\n",
    "            theta[i] = math.atan(abs(l[2] - l[0])/abs(l[3] - l[1]))\n",
    "\n",
    "    # find smallest angle greater than pi/4\n",
    "    optTheta = 100\n",
    "    optIndex = 0\n",
    "    for k in range(len(theta)):\n",
    "        if theta[k] > np.pi/4:\n",
    "            if theta[k] < optTheta:\n",
    "                optTheta = theta[k]\n",
    "                optIndex = k\n",
    "    mostHorz = np.float32(lines[optIndex][0])\n",
    "    # this actually ends up being the least horizontal of the still pretty horizontal lines, deals with noise from extra lines created from the text\n",
    "\n",
    "    # find some points to warp the image, use the midpoint of the lines offset a bit, and the new destination of the endpoints of the least horizontal but still horizontal line\n",
    "    mdpnt = [(mostHorz[0] + mostHorz[2])/2,(mostHorz[1] + mostHorz[3])/2]\n",
    "    adjustment = 100\n",
    "    if mdpnt[1] > 250:\n",
    "        initialPts = np.float32([[mostHorz[0], mostHorz[1]],\n",
    "                                 [mostHorz[2], mostHorz[3]],\n",
    "                                 [mdpnt[0], mdpnt[1] - adjustment]])\n",
    "        finalPts = np.float32([[mostHorz[0], mdpnt[1]],\n",
    "                               [mostHorz[2], mdpnt[1]],\n",
    "                               [mdpnt[0], mdpnt[1] - adjustment]])\n",
    "    else:\n",
    "        initialPts = np.float32([[mostHorz[0], mostHorz[1]],\n",
    "                                 [mostHorz[2], mostHorz[3]],\n",
    "                                 [mdpnt[0], mdpnt[1] + adjustment]])\n",
    "        finalPts = np.float32([[mostHorz[0], mdpnt[1]],\n",
    "                               [mostHorz[2], mdpnt[1]],\n",
    "                               [mdpnt[0], mdpnt[1] + adjustment]])\n",
    "\n",
    "    # denoise it\n",
    "    denoiseImg = cv2.fastNlMeansDenoising(testingImageArray, h=10)\n",
    "\n",
    "    # warp it\n",
    "    M = cv2.getAffineTransform(initialPts, finalPts)\n",
    "    warpedImg = cv2.warpAffine(denoiseImg, M, (denoiseImg.shape[1], denoiseImg.shape[0]))\n",
    "\n",
    "    # red it\n",
    "    redshiftImg = warpedImg\n",
    "    for m in range(warpedImg.shape[0]):\n",
    "        for n in range(warpedImg.shape[1]):\n",
    "            redshiftImg[m][n][1] = 0\n",
    "            redshiftImg[m][n][2] = 0\n",
    "\n",
    "    # tesseract it, also find latency\n",
    "    start = time.time()\n",
    "    wordsRaw = pytesseract.image_to_string(redshiftImg, lang='chi_sim')\n",
    "    stop = time.time()\n",
    "    latency.append(stop - start)\n",
    "\n",
    "    # clean and tokenize the data from OCR\n",
    "    words = wordsRaw.replace(\" \",\"\").splitlines()\n",
    "    testingData = ds['train'][imgNum]['ocr']\n",
    "\n",
    "    pattern = r'\\d+|[\\u4e00-\\u9fff]+|[A-Za-z]+|\\s|[^\\w\\s]'\n",
    "    tokens = []\n",
    "    for i in range(len(words)):\n",
    "        tokens.append(re.findall(pattern, words[i]))\n",
    "\n",
    "    # flatten it out\n",
    "    wordsData = []\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens[i])):\n",
    "            wordsData.append(tokens[i][j])\n",
    "\n",
    "    # run the accuracy checker on the final data\n",
    "    accuracy.append(accuracyChecker(wordsData, testingData))\n",
    "\n",
    "print(np.mean(accuracy))\n",
    "print(np.mean(latency))\n",
    "\n",
    "# still not great"
   ],
   "id": "a4708794b8eadf2e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
