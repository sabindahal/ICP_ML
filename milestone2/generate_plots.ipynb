{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d0ac3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Model 1: Deskewed Red\n",
      "Epoch 1/3, Loss: 9.8193\n",
      "Epoch 2/3, Loss: 9.8085\n",
      "Epoch 3/3, Loss: 9.7808\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: np.str_('萧逸齐男汉1984927湖北省宜昌市伍家岗区965652198409276669')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\classic\\Lib\\site-packages\\sklearn\\utils\\_encode.py:235\u001b[39m, in \u001b[36m_encode\u001b[39m\u001b[34m(values, uniques, check_unknown)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_to_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\classic\\Lib\\site-packages\\sklearn\\utils\\_encode.py:174\u001b[39m, in \u001b[36m_map_to_integer\u001b[39m\u001b[34m(values, uniques)\u001b[39m\n\u001b[32m    173\u001b[39m table = _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m xp.asarray(\u001b[43m[\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m]\u001b[49m, device=device(values))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\classic\\Lib\\site-packages\\sklearn\\utils\\_encode.py:174\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    173\u001b[39m table = _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m xp.asarray([\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values], device=device(values))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\classic\\Lib\\site-packages\\sklearn\\utils\\_encode.py:167\u001b[39m, in \u001b[36m_nandict.__missing__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nan_value\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[31mKeyError\u001b[39m: np.str_('萧逸齐男汉1984927湖北省宜昌市伍家岗区965652198409276669')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 224\u001b[39m\n\u001b[32m    222\u001b[39m model = OCRNet(nclass=nclass).to(DEVICE)\n\u001b[32m    223\u001b[39m train_model(model, train_loader, EPOCHS, SAVE_DIR / fname)\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m acc = \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m lat, thr = evaluate_latency(model, test_ds, preprocess_fn)\n\u001b[32m    226\u001b[39m results[name] = (acc, lat, thr)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 164\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(model, test_loader)\u001b[39m\n\u001b[32m    162\u001b[39m correct, total = \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\classic\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\classic\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\classic\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\classic\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 137\u001b[39m, in \u001b[36mOCRDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    135\u001b[39m proc = \u001b[38;5;28mself\u001b[39m.preprocess_fn(img)\n\u001b[32m    136\u001b[39m x = torch.tensor(proc / \u001b[32m255.0\u001b[39m).unsqueeze(\u001b[32m0\u001b[39m).float()\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m y = torch.tensor(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_encoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel_text\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m])\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x, y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\classic\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:134\u001b[39m, in \u001b[36mLabelEncoder.transform\u001b[39m\u001b[34m(self, y)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) == \u001b[32m0\u001b[39m:\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m xp.asarray([])\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\classic\\Lib\\site-packages\\sklearn\\utils\\_encode.py:237\u001b[39m, in \u001b[36m_encode\u001b[39m\u001b[34m(values, uniques, check_unknown)\u001b[39m\n\u001b[32m    235\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[32m    236\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m check_unknown:\n",
      "\u001b[31mValueError\u001b[39m: y contains previously unseen labels: np.str_('萧逸齐男汉1984927湖北省宜昌市伍家岗区965652198409276669')"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Automated OCR Model Training + Evaluation\n",
    "# Works with lansinuote/ocr_id_card dataset\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Configuration\n",
    "# ------------------------------------------------------------\n",
    "SAVE_DIR = Path(\"saved_model\")\n",
    "SAVE_DIR.mkdir(exist_ok=True)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "IMG_W, IMG_H = 128, 32\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Shared CNN-LSTM Model Definition\n",
    "# ------------------------------------------------------------\n",
    "class OCRNet(nn.Module):\n",
    "    def __init__(self, nclass=80):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.lstm = nn.LSTM(64 * 8, 128, num_layers=2,\n",
    "                             bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(256, nclass)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        b, c, h, w = x.size()\n",
    "        x = x.permute(0, 3, 1, 2).reshape(b, w, -1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "        return x.mean(dim=1)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Preprocessing Functions\n",
    "# ------------------------------------------------------------\n",
    "def deskew_red(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    lines = cv2.HoughLines(edges, 1, np.pi / 180, 200)\n",
    "    angle = 0\n",
    "    if lines is not None:\n",
    "        angles = [theta for rho, theta in lines[:, 0]]\n",
    "        angle = (np.mean(angles) - np.pi / 2) * 180 / np.pi\n",
    "    (h, w) = img.shape[:2]\n",
    "    M = cv2.getRotationMatrix2D((w // 2, h // 2), angle, 1)\n",
    "    img = cv2.warpAffine(img, M, (w, h), flags=cv2.INTER_CUBIC)\n",
    "    b, g, r = cv2.split(img)\n",
    "    red_only = cv2.merge([np.zeros_like(r), np.zeros_like(r), r])\n",
    "    gray = cv2.cvtColor(red_only, cv2.COLOR_BGR2GRAY)\n",
    "    return cv2.resize(gray, (IMG_W, IMG_H))\n",
    "\n",
    "def deskew_grey(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    lines = cv2.HoughLines(edges, 1, np.pi / 180, 200)\n",
    "    angle = 0\n",
    "    if lines is not None:\n",
    "        angles = [theta for rho, theta in lines[:, 0]]\n",
    "        angle = (np.mean(angles) - np.pi / 2) * 180 / np.pi\n",
    "    (h, w) = img.shape[:2]\n",
    "    M = cv2.getRotationMatrix2D((w // 2, h // 2), angle, 1)\n",
    "    img = cv2.warpAffine(img, M, (w, h), flags=cv2.INTER_CUBIC)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return cv2.resize(gray, (IMG_W, IMG_H))\n",
    "\n",
    "def deskew_red_denoise(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    lines = cv2.HoughLines(edges, 1, np.pi / 180, 200)\n",
    "    angle = 0\n",
    "    if lines is not None:\n",
    "        angles = [theta for rho, theta in lines[:, 0]]\n",
    "        angle = (np.mean(angles) - np.pi / 2) * 180 / np.pi\n",
    "    (h, w) = img.shape[:2]\n",
    "    M = cv2.getRotationMatrix2D((w // 2, h // 2), angle, 1)\n",
    "    img = cv2.warpAffine(img, M, (w, h), flags=cv2.INTER_CUBIC)\n",
    "    b, g, r = cv2.split(img)\n",
    "    red_only = cv2.merge([np.zeros_like(r), np.zeros_like(r), r])\n",
    "    gray = cv2.cvtColor(red_only, cv2.COLOR_BGR2GRAY)\n",
    "    denoiseImg = cv2.fastNlMeansDenoising(gray, h=10)\n",
    "    return cv2.resize(denoiseImg, (IMG_W, IMG_H))\n",
    "\n",
    "def predenoise_red(img):\n",
    "    img = cv2.fastNlMeansDenoisingColored(img, None, 10, 10, 7, 21)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    lines = cv2.HoughLines(edges, 1, np.pi / 180, 200)\n",
    "    angle = 0\n",
    "    if lines is not None:\n",
    "        angles = [theta for rho, theta in lines[:, 0]]\n",
    "        angle = (np.mean(angles) - np.pi / 2) * 180 / np.pi\n",
    "    (h, w) = img.shape[:2]\n",
    "    M = cv2.getRotationMatrix2D((w // 2, h // 2), angle, 1)\n",
    "    img = cv2.warpAffine(img, M, (w, h), flags=cv2.INTER_CUBIC)\n",
    "    b, g, r = cv2.split(img)\n",
    "    red_only = cv2.merge([np.zeros_like(r), np.zeros_like(r), r])\n",
    "    gray = cv2.cvtColor(red_only, cv2.COLOR_BGR2GRAY)\n",
    "    return cv2.resize(gray, (IMG_W, IMG_H))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Dataset Wrapper\n",
    "# ------------------------------------------------------------\n",
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, hf_split, label_encoder, preprocess_fn):\n",
    "        self.ds = hf_split\n",
    "        self.label_encoder = label_encoder\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = np.array(self.ds[idx][\"image\"])\n",
    "        ocr_list = self.ds[idx][\"ocr\"]\n",
    "        label_text = \"\".join([w[\"word\"] for w in ocr_list if \"word\" in w and w[\"word\"]])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        proc = self.preprocess_fn(img)\n",
    "        x = torch.tensor(proc / 255.0).unsqueeze(0).float()\n",
    "        y = torch.tensor(self.label_encoder.transform([label_text])[0])\n",
    "        return x, y\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Utility Functions\n",
    "# ------------------------------------------------------------\n",
    "def train_model(model, train_loader, n_epochs, save_path):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {total_loss / len(train_loader.dataset):.4f}\")\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            out = model(x)\n",
    "            preds = out.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return 100 * correct / total\n",
    "\n",
    "def evaluate_latency(model, dataset, preprocess_fn, n=10):\n",
    "    model.eval()\n",
    "    latencies = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(n):\n",
    "            img = np.array(dataset[i][\"image\"])\n",
    "            proc = preprocess_fn(cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "            x = torch.tensor(proc / 255.0).unsqueeze(0).unsqueeze(0).float().to(DEVICE)\n",
    "            t0 = time.perf_counter()\n",
    "            _ = model(x)\n",
    "            t1 = time.perf_counter()\n",
    "            latencies.append((t1 - t0) * 1000)\n",
    "    latency = np.mean(latencies)\n",
    "    throughput = 1000 / latency\n",
    "    return latency, throughput\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load dataset and prepare label encoder\n",
    "# ------------------------------------------------------------\n",
    "train_ds = load_dataset(\"lansinuote/ocr_id_card\", split=\"train[:80%]\")\n",
    "test_ds = load_dataset(\"lansinuote/ocr_id_card\", split=\"train[80%:]\")\n",
    "\n",
    "all_texts = []\n",
    "for sample in train_ds:\n",
    "    text = \"\".join([w[\"word\"] for w in sample[\"ocr\"] if \"word\" in w and w[\"word\"]])\n",
    "    if text:\n",
    "        all_texts.append(text)\n",
    "\n",
    "label_encoder = LabelEncoder().fit(all_texts)\n",
    "nclass = len(label_encoder.classes_)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Train and Evaluate All Models\n",
    "# ------------------------------------------------------------\n",
    "models_info = [\n",
    "    (\"Model 1: Deskewed Red\", deskew_red, \"model1.pt\"),\n",
    "    (\"Model 2: Deskewed Grey\", deskew_grey, \"model2.pt\"),\n",
    "    (\"Model 3: Red+Denoise\", deskew_red_denoise, \"model3.pt\"),\n",
    "    (\"Model 4: PreDenoise Red\", predenoise_red, \"model4.pt\")\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, preprocess_fn, fname in models_info:\n",
    "    print(f\"\\nTraining {name}\")\n",
    "    train_data = OCRDataset(train_ds, label_encoder, preprocess_fn)\n",
    "    test_data = OCRDataset(test_ds, label_encoder, preprocess_fn)\n",
    "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "    model = OCRNet(nclass=nclass).to(DEVICE)\n",
    "    train_model(model, train_loader, EPOCHS, SAVE_DIR / fname)\n",
    "    acc = evaluate_model(model, test_loader)\n",
    "    lat, thr = evaluate_latency(model, test_ds, preprocess_fn)\n",
    "    results[name] = (acc, lat, thr)\n",
    "    print(f\"{name}: Accuracy={acc:.2f}%, Latency={lat:.2f}ms, Throughput={thr:.2f} img/s\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Plot Comparisons\n",
    "# ------------------------------------------------------------\n",
    "models = list(results.keys())\n",
    "accuracy = [v[0] for v in results.values()]\n",
    "latency = [v[1] for v in results.values()]\n",
    "throughput = [v[2] for v in results.values()]\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.set_xlabel(\"Model Version\")\n",
    "ax1.set_ylabel(\"Accuracy (%)\", color=\"tab:blue\")\n",
    "ax1.plot(models, accuracy, \"o-\", color=\"tab:blue\", label=\"Accuracy\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"Latency (ms)\", color=\"tab:red\")\n",
    "ax2.plot(models, latency, \"s--\", color=\"tab:red\", label=\"Latency\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"tab:red\")\n",
    "\n",
    "plt.title(\"Model Accuracy vs Latency\")\n",
    "fig.autofmt_xdate(rotation=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.set_xlabel(\"Model Version\")\n",
    "ax1.set_ylabel(\"Accuracy (%)\", color=\"tab:blue\")\n",
    "ax1.plot(models, accuracy, \"o-\", color=\"tab:blue\", label=\"Accuracy\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"Throughput (images/sec)\", color=\"tab:green\")\n",
    "ax2.plot(models, throughput, \"d--\", color=\"tab:green\", label=\"Throughput\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"tab:green\")\n",
    "\n",
    "plt.title(\"Model Accuracy vs Throughput\")\n",
    "fig.autofmt_xdate(rotation=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7163f186",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
